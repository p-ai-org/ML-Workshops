{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise notebook for the fourth session\n",
    "\n",
    "This is the exercise notebook for the fourth session of the [Machine Learning workshop series at Harvey Mudd College](http://www.aashitak.com/ML-Workshops/). Please feel free to ask for help from the instructor and/or TAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's exercise, we will work with the [Titanic dataset from Kaggle](https://www.kaggle.com/c/titanic). The objective of this Kaggle competition is to predict whether a passenger survives or not given a number of features related to passengers' information such as gender, age, ticket class, etc. We are going to build a few classification models to predict whether a passenger survives. The `train.csv` file contains features along with the information about the survival of the passenger, so we will use it to train and validate our models. The `test.csv` file contains only features and we will use one of our trained models to predict the survival for these passengers and [submit our predictions to the competitions leaderboard](https://www.kaggle.com/c/titanic/submit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, the data preprocessing and feature engineering that we did in the previous sessions is summarized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'titanic/'\n",
    "df = pd.read_csv(path + 'train.csv')\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "target = train.Survived.astype('category', ordered=False)\n",
    "train.drop('Survived', axis=1)\n",
    "\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "PassengerId = test.PassengerId\n",
    "\n",
    "def get_Titles(df):\n",
    "    df.Name = df.Name.apply(lambda name: re.findall(\"\\s\\S+[.]\\s\", name)[0].strip())\n",
    "    df = df.rename(columns = {'Name': 'Title'})\n",
    "    df.Title.replace({'Ms.': 'Miss.', 'Mlle.': 'Miss.', 'Dr.': 'Rare', 'Mme.': 'Mr.', 'Major.': 'Rare', 'Lady.': 'Rare', 'Sir.': 'Rare', 'Col.': 'Rare', 'Capt.': 'Rare', 'Countess.': 'Rare', 'Jonkheer.': 'Rare', 'Dona.': 'Rare', 'Don.': 'Rare', 'Rev.': 'Rare'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def fill_Age(df):\n",
    "    df.Age = df.Age.fillna(df.groupby(\"Title\").Age.transform(\"median\"))\n",
    "    return df\n",
    "\n",
    "def get_Group_size(df):\n",
    "    Ticket_counts = df.Ticket.value_counts()\n",
    "    df['Ticket_counts'] = df.Ticket.apply(lambda x: Ticket_counts[x])\n",
    "    df['Family_size'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['Group_size'] = df[['Family_size', 'Ticket_counts']].max(axis=1)\n",
    "    return df\n",
    "\n",
    "def process_features(df):\n",
    "    df.Sex = df.Sex.astype('category', ordered=False).cat.codes\n",
    "    features_to_keep = ['Age', 'Fare', 'Group_size', 'Pclass', 'Sex']\n",
    "    df = df[features_to_keep]\n",
    "    return df\n",
    "\n",
    "def process_data(df):\n",
    "    df = df.copy()\n",
    "    df = get_Titles(df)\n",
    "    df = fill_Age(df)\n",
    "    df = get_Group_size(df)\n",
    "    df = process_features(df)\n",
    "    medianFare = df['Fare'].median()\n",
    "    df['Fare'] = df['Fare'].fillna(medianFare)\n",
    "    return df\n",
    "\n",
    "X_train, X_test = process_data(train), process_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please feel free to refer to the classification algorithms notebook for the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, split the data into training and validation set using `train_test_split` and name the variables as `X_train, X_valid, y_train, y_valid `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Group_size</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>28.0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>30.0</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>45.0</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age     Fare  Group_size  Pclass  Sex\n",
       "105  28.0   7.8958           1       3    1\n",
       "68   17.0   7.9250           7       3    0\n",
       "253  30.0  16.1000           2       3    1\n",
       "320  22.0   7.2500           1       3    1\n",
       "706  45.0  13.5000           1       2    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105    0\n",
       "68     1\n",
       "253    0\n",
       "320    0\n",
       "706    1\n",
       "Name: Survived, dtype: category\n",
       "Categories (2, int64): [0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression classifier on `X_train, y_train` and test its accuracy on both `X_train, y_train` and `X_valid, y_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.80\n",
      "Accuracy of Logistic regression classifier on test set: 0.78\n"
     ]
    }
   ],
   "source": [
    "LR_clf = LogisticRegression()\n",
    "LR_clf.fit(X_train, y_train)\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(LR_clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on validation set: {:.2f}'\n",
    "     .format(LR_clf.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The evaluation metric for this competition is accuracy](https://www.kaggle.com/c/titanic/overview/evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training  a few more classifiers and compare the accuracy. Try tuning the hyperparameters too. You can also try more feature engineering by editing the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector classifier on training set: 0.91\n",
      "Accuracy of Support Vector classifier on test set: 0.72\n"
     ]
    }
   ],
   "source": [
    "svc_clf = SVC().fit(X_train, y_train)\n",
    "print('Accuracy of Support Vector classifier on training set: {:.2f}'\n",
    "     .format(svc_clf.score(X_train, y_train)))\n",
    "print('Accuracy of Support Vector classifier on validation set: {:.2f}'\n",
    "     .format(svc_clf.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_MLP(X_train, y_train, input_dim, dropout, epochs, batch_size):\n",
    "    print('-----------Running Multilayer Perceptron-----------')\n",
    "    # Build model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Choose optimizer and loss function\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "    loss = 'binary_crossentropy'\n",
    "    # Compile \n",
    "    model.compile(optimizer=opt, \n",
    "        loss=loss,\n",
    "        metrics=['accuracy'])\n",
    "    # Fit on training data and cross-validate\n",
    "    model.fit(X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Running Multilayer Perceptron-----------\n",
      "Train on 668 samples\n",
      "Epoch 1/70\n",
      "668/668 [==============================] - 0s 538us/sample - loss: 1.2518 - accuracy: 0.5614\n",
      "Epoch 2/70\n",
      "668/668 [==============================] - 0s 25us/sample - loss: 0.8635 - accuracy: 0.6377\n",
      "Epoch 3/70\n",
      "668/668 [==============================] - 0s 22us/sample - loss: 0.8470 - accuracy: 0.5734\n",
      "Epoch 4/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.8269 - accuracy: 0.6213\n",
      "Epoch 5/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.7446 - accuracy: 0.6452\n",
      "Epoch 6/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.6887 - accuracy: 0.6377\n",
      "Epoch 7/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6845 - accuracy: 0.6452\n",
      "Epoch 8/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.6580 - accuracy: 0.6527\n",
      "Epoch 9/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.6493 - accuracy: 0.6781\n",
      "Epoch 10/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.6282 - accuracy: 0.6437\n",
      "Epoch 11/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6352 - accuracy: 0.6512\n",
      "Epoch 12/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.6011 - accuracy: 0.6587\n",
      "Epoch 13/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6249 - accuracy: 0.6751\n",
      "Epoch 14/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6094 - accuracy: 0.6796\n",
      "Epoch 15/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.6172 - accuracy: 0.6542\n",
      "Epoch 16/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6410 - accuracy: 0.6677\n",
      "Epoch 17/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.6182 - accuracy: 0.6707\n",
      "Epoch 18/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6110 - accuracy: 0.6751\n",
      "Epoch 19/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5953 - accuracy: 0.6617\n",
      "Epoch 20/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.6264 - accuracy: 0.6722\n",
      "Epoch 21/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5946 - accuracy: 0.6766\n",
      "Epoch 22/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5869 - accuracy: 0.6961\n",
      "Epoch 23/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5930 - accuracy: 0.6722\n",
      "Epoch 24/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6110 - accuracy: 0.6602\n",
      "Epoch 25/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.6057 - accuracy: 0.7006\n",
      "Epoch 26/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.6067 - accuracy: 0.6766\n",
      "Epoch 27/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5941 - accuracy: 0.7141\n",
      "Epoch 28/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5921 - accuracy: 0.6766\n",
      "Epoch 29/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5800 - accuracy: 0.6707\n",
      "Epoch 30/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5870 - accuracy: 0.6886\n",
      "Epoch 31/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5803 - accuracy: 0.6751\n",
      "Epoch 32/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5708 - accuracy: 0.7231\n",
      "Epoch 33/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5880 - accuracy: 0.6766\n",
      "Epoch 34/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5688 - accuracy: 0.7186\n",
      "Epoch 35/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5825 - accuracy: 0.6931\n",
      "Epoch 36/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5861 - accuracy: 0.7320\n",
      "Epoch 37/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5819 - accuracy: 0.6961\n",
      "Epoch 38/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5493 - accuracy: 0.7111\n",
      "Epoch 39/70\n",
      "668/668 [==============================] - 0s 23us/sample - loss: 0.5699 - accuracy: 0.7201\n",
      "Epoch 40/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5595 - accuracy: 0.7290\n",
      "Epoch 41/70\n",
      "668/668 [==============================] - 0s 23us/sample - loss: 0.5421 - accuracy: 0.7320\n",
      "Epoch 42/70\n",
      "668/668 [==============================] - 0s 23us/sample - loss: 0.5569 - accuracy: 0.7216\n",
      "Epoch 43/70\n",
      "668/668 [==============================] - 0s 23us/sample - loss: 0.5588 - accuracy: 0.7216\n",
      "Epoch 44/70\n",
      "668/668 [==============================] - 0s 23us/sample - loss: 0.5429 - accuracy: 0.7425\n",
      "Epoch 45/70\n",
      "668/668 [==============================] - 0s 23us/sample - loss: 0.5471 - accuracy: 0.7171\n",
      "Epoch 46/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5434 - accuracy: 0.7395\n",
      "Epoch 47/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5369 - accuracy: 0.7171\n",
      "Epoch 48/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5386 - accuracy: 0.7470\n",
      "Epoch 49/70\n",
      "668/668 [==============================] - 0s 19us/sample - loss: 0.5427 - accuracy: 0.7425\n",
      "Epoch 50/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5298 - accuracy: 0.7410\n",
      "Epoch 51/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5141 - accuracy: 0.7575\n",
      "Epoch 52/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5316 - accuracy: 0.7620\n",
      "Epoch 53/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5568 - accuracy: 0.6991\n",
      "Epoch 54/70\n",
      "668/668 [==============================] - 0s 22us/sample - loss: 0.5315 - accuracy: 0.7545\n",
      "Epoch 55/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5559 - accuracy: 0.7246\n",
      "Epoch 56/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5401 - accuracy: 0.7560\n",
      "Epoch 57/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5225 - accuracy: 0.7575\n",
      "Epoch 58/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5175 - accuracy: 0.7395\n",
      "Epoch 59/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5301 - accuracy: 0.7560\n",
      "Epoch 60/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5365 - accuracy: 0.7590\n",
      "Epoch 61/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5168 - accuracy: 0.7620\n",
      "Epoch 62/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5162 - accuracy: 0.7680\n",
      "Epoch 63/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5437 - accuracy: 0.7545\n",
      "Epoch 64/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5067 - accuracy: 0.7680\n",
      "Epoch 65/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5040 - accuracy: 0.7784\n",
      "Epoch 66/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.5247 - accuracy: 0.7530\n",
      "Epoch 67/70\n",
      "668/668 [==============================] - 0s 22us/sample - loss: 0.5255 - accuracy: 0.7380\n",
      "Epoch 68/70\n",
      "668/668 [==============================] - 0s 23us/sample - loss: 0.5096 - accuracy: 0.7799\n",
      "Epoch 69/70\n",
      "668/668 [==============================] - 0s 21us/sample - loss: 0.5084 - accuracy: 0.7769\n",
      "Epoch 70/70\n",
      "668/668 [==============================] - 0s 20us/sample - loss: 0.4999 - accuracy: 0.7740\n",
      "223/223 [==============================] - 0s 345us/sample - loss: 0.4639 - accuracy: 0.7982\n"
     ]
    }
   ],
   "source": [
    "model = run_MLP(X_train, y_train, 5, 0.1, 70, 128)\n",
    "score = model.evaluate(X_valid, y_valid, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters using gridsearch (this is why we have the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have explored a different classifiers and decided on one trained model (or a voting classifer ensemble as seen before), let us use it to make predictions using the features from `X_test` and save the results into `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "y_test = [1 if i >= 0.5 else 0 for i in y_test]\n",
    "y_test = pd.Series((i for i in y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe for submission using the predictions from `y_test` and save it to a csv file. It is important that our submission file is in correct format to be graded without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PassengerId': PassengerId, 'Survived': y_test})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
