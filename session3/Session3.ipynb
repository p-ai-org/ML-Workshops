{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3\n",
    "\n",
    "This is the notebook for the third session of the Fall 2019 [P-ai](http://www.p-ai.org) workshops.\n",
    "It covers regression, classification, and some neural networks.\n",
    "\n",
    "This is adapted from [Aashita Kesharwani's workshop series at Harvey Mudd.](http://www.aashitak.com/ML-Workshops/)\n",
    "\n",
    "## Topics\n",
    "- Linear Regression\n",
    "- Ways to avoid overfitting\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- k-Nearest Neighbours\n",
    "- Support Vector Machines\n",
    "- Random Forest ensemble\n",
    "- Voting Classifiers\n",
    "- Feedforward Neural Networks\n",
    "\n",
    "## Part 1: Linear Regression\n",
    "- Determining the impact of response variables on the target variable. \n",
    "- Fitting a curve using training data to estimate target variable for unseen data\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8a/Gaussian_kernel_regression.png\" width=\"300\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Regression curve </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of linear regression using the [diabetes dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset) from [sklearn.datasets](https://scikit-learn.org/stable/datasets/index.html). First we import python modules and the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019908 -0.017646  \n",
       "1 -0.039493 -0.068330 -0.092204  \n",
       "2 -0.002592  0.002864 -0.025930  \n",
       "3  0.034309  0.022692 -0.009362  \n",
       "4 -0.002592 -0.031991 -0.046641  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has many features (or variables), but we will pick only the BMI so as to be able to plot and visualize the relationship between input and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df['bmi'].values\n",
    "Y = diabetes.target\n",
    "\n",
    "plt.scatter(X, Y);\n",
    "plt.xlabel('Body mass index (BMI)');\n",
    "plt.ylabel('Disease progression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're trying to find the optimal value for the slope and intercept for the line and we are learning from the training examples. Our objective is to minimize the difference between the actual $y$ value and the value predicted using the line. \n",
    "\n",
    "<img src=\"http://www.ken-szulczyk.com/misc/statistics/linear_regression.png\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055229</td>\n",
       "      <td>173.0</td>\n",
       "      <td>65.229334</td>\n",
       "      <td>107.770666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003494</td>\n",
       "      <td>129.0</td>\n",
       "      <td>13.494355</td>\n",
       "      <td>115.505645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.024529</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-14.528759</td>\n",
       "      <td>124.528759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094031</td>\n",
       "      <td>263.0</td>\n",
       "      <td>104.030569</td>\n",
       "      <td>158.969431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.056863</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-46.863122</td>\n",
       "      <td>114.863122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.021295</td>\n",
       "      <td>107.0</td>\n",
       "      <td>-11.295323</td>\n",
       "      <td>118.295323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.041774</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-31.773753</td>\n",
       "      <td>134.773753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.047685</td>\n",
       "      <td>258.0</td>\n",
       "      <td>57.684650</td>\n",
       "      <td>200.315350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.045529</td>\n",
       "      <td>202.0</td>\n",
       "      <td>55.529025</td>\n",
       "      <td>146.470975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.045007</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-35.007189</td>\n",
       "      <td>128.007189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x      y      y_pred       error\n",
       "0  0.055229  173.0   65.229334  107.770666\n",
       "1  0.003494  129.0   13.494355  115.505645\n",
       "2 -0.024529  110.0  -14.528759  124.528759\n",
       "3  0.094031  263.0  104.030569  158.969431\n",
       "4 -0.056863   68.0  -46.863122  114.863122\n",
       "5 -0.021295  107.0  -11.295323  118.295323\n",
       "6 -0.041774  103.0  -31.773753  134.773753\n",
       "7  0.047685  258.0   57.684650  200.315350\n",
       "8  0.045529  202.0   55.529025  146.470975\n",
       "9 -0.045007   93.0  -35.007189  128.007189"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 1000\n",
    "b = 10\n",
    "import random\n",
    "random.seed(0)\n",
    "idx = random.sample(range(len(df)), 10)\n",
    "x, y = X[idx], Y[idx]\n",
    "y_pred = w*x + b\n",
    "error = y - y_pred\n",
    "pd.DataFrame({'x': x, 'y': y, 'y_pred': y_pred, \n",
    "              'error': error})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple linear regression (linear regression with one variable) is formulated as $ y_{pred} = w * x + b $.\n",
    "\n",
    "To find the optimal values for $w$ and $b$, we need to quantify the cost function (also known as the error function or the loss function) that we can minimize. \n",
    "\n",
    "* How do we formulate it?\n",
    "* Should we sum up the errors? If not, why?\n",
    "\n",
    "The simple linear regression model uses the mean-squared error (MSE) as the cost function. We square the errors and then take their average.\n",
    "\n",
    "$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 $$\n",
    "\n",
    "The [gradient descent algorithm](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) is used to update the weights iteratively in the direction of the steepest descent of the cost function. \n",
    "\n",
    "$$ w := w - \\alpha \\nabla J $$\n",
    "\n",
    "where $\\nabla J$ is the gradient of the cost function $J$ and $\\alpha$ is the learning rate that determines the size of steps that we take descending on the path of gradient.\n",
    "\n",
    "<img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" width=\"350\" height=\"450\" />\n",
    "<p style=\"text-align: center;\"> Minimizing the cost function using gradient descent </p> \n",
    "\n",
    "\n",
    "To derive the formula for updating weights using gradient descent, we first substitute $ y_{pred}^{(i)} = w * x^{(i)}+b $ in $J$ to get\n",
    "\n",
    "$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - (w * x^{(i)}+b))^2 $$\n",
    "\n",
    "Then we take the partial derivative, \n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})   \\ x^{(i)} $$\n",
    "Thus, we get\n",
    "$$ w := w - \\alpha \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})   \\ x^{(i)} $$\n",
    "Similarly,\n",
    "$$ b := b - \\alpha \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})   $$\n",
    "\n",
    "To summarize, we defined a cost function to quantify the error in predicting outputs and then we update the weights so as to minimize the cost in the fastest way with the help of gradient descent algorithm.\n",
    "\n",
    "The same formulation and understanding can be extended to linear regression with more than one variable, say $x_1, x_2, \\dots, x_n$ with the equation $ y_{pred} = b + w_1 * x_1 + w_2 * x_2 + \\cdots + w_n * x_n$. And we estimate the weights $w_1, w_2, \\dots, w_n$ corresponding to each variable as well as the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has an implementation of the linear regression as demonstrated below. First we import the function [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and then initialize the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the regressor using the `fit()` method on the smaller set of 10 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(-1, 1)\n",
    "lin_reg.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1266.9047008504815, 143.44201545455365)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = lin_reg.coef_[0]\n",
    "b = lin_reg.intercept_\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_line(w, b):\n",
    "    x_values = np.linspace(X.min(), X.max(), 100)\n",
    "    y_values = w*x_values + b\n",
    "    plt.plot(x_values, y_values, 'r-')\n",
    "\n",
    "plt.scatter(x, y);\n",
    "plot_line(w, b);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the regressor on the entire dataset and fit a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = X.reshape(-1, 1)\n",
    "lin_reg.fit(X, Y)\n",
    "w = lin_reg.coef_[0]\n",
    "b = lin_reg.intercept_\n",
    "plt.scatter(X, Y);\n",
    "plot_line(w, b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature BMI does not seem to possess a linear relationship with the disease progression. The coefficient of determination (denoted by $R^2$) is a statistical measure of how close the data are to the fitted regression line and it can be calculated using `score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3439237602253803"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.score(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the R-sqaured is around 34%, which is somewhat low.\n",
    "\n",
    "$R^2$ is one of the metric for evaluating regression models. Following are some data points plots for demonstration, the first two have $R^2$ equal to 1 and the last two with $R^2$ equal to 0.\n",
    "\n",
    "\n",
    "<div style=\"display:flex\">\n",
    "     <div style=\"flex:1;padding-right:5px;\">\n",
    "          <img src=\"http://www.ken-szulczyk.com/misc/statistics/correlation_01.gif\" width=\"200\" height=\"200\">\n",
    "     </div>\n",
    "     <div style=\"flex:1;padding-left:5px;\">\n",
    "          <img src=\"http://www.ken-szulczyk.com/misc/statistics/correlation_02.gif\" width=\"200\" height=\"200\">\n",
    "     </div>\n",
    "     <div style=\"flex:1;padding-left:5px;\">\n",
    "          <img src=\"http://www.ken-szulczyk.com/misc/statistics/correlation_03.gif\" width=\"400\" height=\"400\">\n",
    "     </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: \n",
    "- Predicting a label to classify the data points.\n",
    "- Finding a decision boundary using a labeled training dataset to determine labels for unseen data.\n",
    "\n",
    "An example of binary classification algorithm: Logistic regression - separates the classes using a linear boundary \n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/f663cd4f29335972950dded4d422c07aeee8af55/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Logistic Regression classifier </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three classifiers A, B and C are trained on a given labeled dataset. The accuracy of the trained classifiers in predicting the labels correctly on the same dataset is as follows.\n",
    "\n",
    "|Models | Accuracy| \n",
    "|---|---|\n",
    "| Model A | 90%|\n",
    "| Model B | 80%|\n",
    "| Model C | 70%|\n",
    "\n",
    "Clearly, model A is better at predicting labels for the training data than model B and C. Do you think model A will do a better job in predicting labels for yet unseen data as well?\n",
    "\n",
    "To answer this question, let us first briefly overwiew the learning process for logistic classifiers.\n",
    "\n",
    "It will be helpful to build our understanding of the learning process based on either the logistic classifier or the linear regression. When the neural networks are pared-down to a single layer with the commonly used logistic sigmoid function as the activation function, then what we get is a logistic classifier. These algorithms, that is logistic/linear regression and simple neural networks, are called parametric machine learning algorithms. In the context of parameteric algorithms, let us think about the following questions related to the learning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the machine learn?\n",
    "\n",
    "Answer: Model parameters (or weights) specific for each classification/regression algorithm.\n",
    "\n",
    "#### How does the machine learn the parameters (or weights)?\n",
    "\n",
    "Model parameters (or weights) are updated to keep on ***minimizing the cost function*** iteratively using the training data. The cost has distinctive mathematical formulations for various algorithms but the gist of the learning process is the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function  \n",
    "* For regression: Measures the difference between the predicted output and the true output in the ***training datset***. \n",
    "* For classification: Measures the error in classifying examples in the ***training dataset***. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***When to stop the iterative learning process? Until the cost function has reached its minimum value?*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question, let us consider this binary classification problem with two variables (features). \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg\" width=\"250\" height=\"250\" />\n",
    "\n",
    "* Which of the two decision boundaries (black or green) will have a lower value for the cost function?\n",
    "* Which decision boundary would you prefer for classifying the unseen examples?\n",
    "\n",
    "Since the cost function is calculated solely based on the training dataset, minimizing it too much might mean that the model do not generalize well to unseen examples. This is called overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Over-fitting and under-fitting to the training set***  \n",
    "The models can over-train on a dataset, that is they learn the dataset so well that they do not generalize well to the examples outside of that dataset. \n",
    "\n",
    "If we try to fit too complex of a curve as the decision boundary separating the classes and we don't have enough training examples to estimate the parameters for the curve, then we suffer from over-fitting.\n",
    "\n",
    "On the other hand, if we try separating the classes with an over-simplified curve as the decision boundary and we have enough training examples to estimate a curve that would be a better fit, then we suffer from under-fitting. \n",
    "\n",
    "<img src=\"https://vitalflux.com/wp-content/uploads/2015/02/fittings.jpg\" width=\"600\" height=\"800\" />\n",
    "\n",
    "How do we know whether our model is overfitting or underfitting to the training set?\n",
    "\n",
    "Answer: At the beginning, we save some examples as the validation set and use it to test the performance of the model. \n",
    "\n",
    "|Models | Accuracy on the training set | Accuracy on the validation set | \n",
    "|---|---|---|\n",
    "| Model A | 90%| 70% |\n",
    "| Model B | 80%| 75% |\n",
    "| Model C | 70%| 65% |\n",
    "\n",
    "* With this additional information, can you guess which model will likely perform better for the unseen data?\n",
    "* Which of these three models would you suspect for overfitting to the training data?\n",
    "* Which of these three models would you suspect for underfitting to the training data?\n",
    "\n",
    "The problem for over-fitting and under-fitting and the underlying reasons of model complexity is the same for regression as well.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/machine-learning/latest/dg/images/mlconcepts_image5.png\" width=\"600\" height=\"800\" />\n",
    "\n",
    "#### Key take-aways so far:\n",
    "- Always save some examples from the datasets for testing model performance.\n",
    "- Pay attention to the model performance on the validation set rather than solely on the training set.\n",
    "- Watch out for both under-fitting and over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### How to address overfitting?\n",
    "- Reduce the number of features \n",
    "    - Discard some features\n",
    "    - Dimensionality reduction techniques such PCA, LDA, etc.\n",
    "- Simplify the model (by tuning hyperparameters)\n",
    "- Early termination (reducing the number of iterations for training)\n",
    "- Regularization, if applicable\n",
    "- Add more training examples, if possible  \n",
    "<img src=\"https://i.stack.imgur.com/rpqa6.jpg\" width=\"450\" height=\"600\" />\n",
    "\n",
    "In a nutshell, to reduce overfitting, reduce complexity.\n",
    "To reduce underfitting, enhance complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand/overview) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  \n",
       "0        81        0.0       3          13     16  \n",
       "1        80        0.0       8          32     40  \n",
       "2        80        0.0       5          27     32  \n",
       "3        75        0.0       3          10     13  \n",
       "4        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "path = 'bike-sharing-demand/'\n",
    "\n",
    "rides = pd.read_csv(path + 'train.csv')\n",
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fields**\n",
    "\n",
    "* datetime - hourly date + timestamp    \n",
    "* season -  1 = spring, 2 = summer, 3 = fall, 4 = winter   \n",
    "* holiday - whether the day is considered a holiday  \n",
    "* workingday - whether the day is neither a weekend nor holiday  \n",
    "* weather -   \n",
    "    1: Clear, Few clouds, Partly cloudy, Partly cloudy   \n",
    "    2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist   \n",
    "    3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds   \n",
    "    4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog   \n",
    "* temp - temperature in Celsius  \n",
    "* atemp - \"feels like\" temperature in Celsius  \n",
    "* humidity - relative humidity  \n",
    "* windspeed - wind speed  \n",
    "* casual - number of non-registered user rentals initiated  \n",
    "* registered - number of registered user rentals initiated  \n",
    "* count - number of total rentals  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the *datetime* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2011-01-01 00:00:00', '2011-01-01 01:00:00',\n",
       "       '2011-01-01 02:00:00', '2011-01-01 03:00:00',\n",
       "       '2011-01-01 04:00:00'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides['datetime'].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform some feature engineering and data pre-processing similar to what we practised in the previous two sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# We extract 'month', 'hour', 'weekday' from the 'datetime' column\n",
    "def extract_from_datetime(rides):\n",
    "    rides[\"date\"] = rides[\"datetime\"].apply(lambda x : x.split()[0])\n",
    "    rides[\"hour\"] = rides[\"datetime\"].apply(lambda x : x.split()[1].split(\":\")[0])\n",
    "    rides[\"weekday\"] = rides[\"date\"].apply(lambda dateString : \n",
    "                            datetime.strptime(dateString,\"%Y-%m-%d\").weekday())\n",
    "    rides[\"month\"] = rides[\"date\"].apply(lambda dateString : \n",
    "                            datetime.strptime(dateString,\"%Y-%m-%d\").month)\n",
    "    return rides\n",
    "\n",
    "# We one-hot encode the categorical features\n",
    "def one_hot_encoding(rides):\n",
    "    dummy_fields = ['season', 'weather', 'month', 'hour', 'weekday']\n",
    "    for each in dummy_fields:\n",
    "        dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "        rides = pd.concat([rides, dummies], axis=1)\n",
    "    return rides\n",
    "\n",
    "# We drop the columns that are redundant now\n",
    "def drop_features(rides):\n",
    "    features_to_drop = ['datetime', 'date', \n",
    "                        'month', 'hour', 'weekday', \n",
    "                        'season', 'weather']\n",
    "\n",
    "    rides = rides.drop(features_to_drop, axis=1)\n",
    "    return rides\n",
    "\n",
    "# Now we aggregate all the above defined functions inside a function\n",
    "def feature_engineering(rides):\n",
    "    rides = extract_from_datetime(rides)\n",
    "    rides = one_hot_encoding(rides)\n",
    "    rides = drop_features(rides)\n",
    "    return rides\n",
    "\n",
    "# Now we apply all the above defined functions to the rides dataframe\n",
    "rides = feature_engineering(rides)\n",
    "rides = rides[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we defined all the steps as functions and bundled them into another function `feature_engineering` is so as to reuse the code for processing the data from `test.csv` file for which we will make predictions at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "      <th>season_1</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_21</th>\n",
       "      <th>hour_22</th>\n",
       "      <th>hour_23</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.84</td>\n",
       "      <td>12.880</td>\n",
       "      <td>75</td>\n",
       "      <td>6.0032</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.58</td>\n",
       "      <td>19.695</td>\n",
       "      <td>76</td>\n",
       "      <td>16.9979</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.04</td>\n",
       "      <td>21.970</td>\n",
       "      <td>77</td>\n",
       "      <td>19.9995</td>\n",
       "      <td>40</td>\n",
       "      <td>70</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.40</td>\n",
       "      <td>20.455</td>\n",
       "      <td>87</td>\n",
       "      <td>16.9979</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    holiday  workingday   temp   atemp  humidity  windspeed  casual  \\\n",
       "0         0           0   9.84  14.395        81     0.0000       3   \n",
       "5         0           0   9.84  12.880        75     6.0032       0   \n",
       "10        0           0  15.58  19.695        76    16.9979      12   \n",
       "15        0           0  18.04  21.970        77    19.9995      40   \n",
       "20        0           0  16.40  20.455        87    16.9979      11   \n",
       "\n",
       "    registered  count  season_1    ...      hour_21  hour_22  hour_23  \\\n",
       "0           13     16         1    ...            0        0        0   \n",
       "5            1      1         1    ...            0        0        0   \n",
       "10          24     36         1    ...            0        0        0   \n",
       "15          70    110         1    ...            0        0        0   \n",
       "20          25     36         1    ...            0        0        0   \n",
       "\n",
       "    weekday_0  weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  \\\n",
       "0           0          0          0          0          0          1   \n",
       "5           0          0          0          0          0          1   \n",
       "10          0          0          0          0          0          1   \n",
       "15          0          0          0          0          0          1   \n",
       "20          0          0          0          0          0          1   \n",
       "\n",
       "    weekday_6  \n",
       "0           0  \n",
       "5           0  \n",
       "10          0  \n",
       "15          0  \n",
       "20          0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed',\n",
       "       'season_1', 'season_2', 'season_3', 'season_4', 'weather_1',\n",
       "       'weather_2', 'weather_3', 'weather_4', 'month_1', 'month_2', 'month_3',\n",
       "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
       "       'month_10', 'month_11', 'month_12', 'hour_00', 'hour_01', 'hour_02',\n",
       "       'hour_03', 'hour_04', 'hour_05', 'hour_06', 'hour_07', 'hour_08',\n",
       "       'hour_09', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14',\n",
       "       'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20',\n",
       "       'hour_21', 'hour_22', 'hour_23', 'weekday_0', 'weekday_1', 'weekday_2',\n",
       "       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2178, 57)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all algorithms using gradient descent for minimizing the cost function, normalizing features helps speed up the learning process. This is to make each feature equally important. We substract the quantitative features by their mean and divide by their standard deviation to redistribute them to have mean 0 and standard deviation 1. \n",
    "$$ x' = \\frac{x - \\mu}{\\sigma} $$\n",
    "![](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2018-01-23-at-2.27.20-PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_features = ['temp', 'atemp', 'humidity', 'windspeed']\n",
    "\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quantitative_features:\n",
    "    mean, std = rides[each].mean(), rides[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    rides.loc[:, each] = (rides[each] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['casual' 'registered' 'count'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-de1a23c4f460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Next we extract the target variables from the dataframe (what we're predicting!)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrides\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'casual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'registered'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'casual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'registered'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2726\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['casual' 'registered' 'count'] not in index\""
     ]
    }
   ],
   "source": [
    "# Next we extract the target variables from the dataframe (what we're predicting!)\n",
    "target = rides[['casual', 'registered', 'count']]\n",
    "target = np.log1p(target)\n",
    "rides = rides.drop(['casual', 'registered', 'count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we split the data into training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(rides, target,\n",
    "                                        random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear regression model using the training set and calculate the $R^2$ score for both training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8133152324657984\n",
      "0.7925408119102534\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(lin_reg.score(X_train, y_train))\n",
    "print(lin_reg.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following $R^2$ values for the training and validation set.  \n",
    "`R-squared score (training): 0.809\n",
    "R-squared score (validation): 0.803"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try polynomial regression with degree 2. First we get polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2 = PolynomialFeatures(degree=2)\n",
    "X_poly2 = poly2.fit_transform(rides)\n",
    "X_train_poly2, X_valid_poly2, y_train_poly2, y_valid_poly2 = train_test_split(X_poly2, \n",
    "                                                    target['count'], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a polynomial regression model using [`LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on polynomial features and call it `polyreg2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9643428814854212\n",
      "-2.673965272128968e+21\n"
     ]
    }
   ],
   "source": [
    "polyreg2 = LinearRegression().fit(X_train_poly2, y_train_poly2)\n",
    "print(polyreg2.score(X_train_poly2, y_train_poly2))\n",
    "print(polyreg2.score(X_valid_poly2, y_valid_poly2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a polynomial regression coupled with Ridge and call it `polyreg2_ridge`. Tune the regularization parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9632616779716651\n",
      "0.8961177233450143\n"
     ]
    }
   ],
   "source": [
    "polyreg2_ridge = Ridge(alpha=1).fit(X_train_poly2, y_train_poly2)\n",
    "print(polyreg2_ridge.score(X_train_poly2, y_train_poly2))\n",
    "print(polyreg2_ridge.score(X_valid_poly2, y_valid_poly2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try polynomial regression with degree 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly3 = PolynomialFeatures(degree=3)\n",
    "X_poly3 = poly3.fit_transform(rides)\n",
    "X_train_poly3, X_valid_poly3, y_train_poly3, y_valid_poly3 = train_test_split(X_poly3, \n",
    "                                                    target['count'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 1.000\n",
      "R-squared score (validation): 0.771\n"
     ]
    }
   ],
   "source": [
    "polyreg3 = LinearRegression().fit(X_train_poly3, y_train_poly3)\n",
    "\n",
    "polyreg3_train_score = polyreg3.score(X_train_poly3, y_train_poly3)\n",
    "polyreg3_valid_score = polyreg3.score(X_valid_poly3, y_valid_poly3)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(polyreg3_train_score))\n",
    "print('R-squared score (validation): {:.3f}'\n",
    "     .format(polyreg3_valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests the model has overfitted to the training set excessively. Nonetheless, the very high $R^2$ looks promising, so we use regularization on the the polynomial regression with degree 3 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the polynomial regression for degree 3 coupled with Ridge and call it `polyreg3_ridge`. Tune the regularization parameter alpha starting with a not so high value, say 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyreg3_ridge = Ridge(alpha=25).fit(X_train_poly3, y_train_poly3)\n",
    "polyreg3_train_score = polyreg3_ridge.score(X_train_poly3, y_train_poly3)\n",
    "polyreg3_valid_score = polyreg3_ridge.score(X_valid_poly3, y_valid_poly3)\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(polyreg3_train_score))\n",
    "print('R-squared score (validation): {:.3f}'\n",
    "     .format(polyreg3_valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following function for the root mean-squared error (RMSE), compare the different regression models, preferably by plotting a graph. Similarly, plot a graph to compare the $R^2$ scores as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(reg):\n",
    "    y_pred_train = reg.predict(X_train_poly)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n",
    "    y_pred_valid = reg.predict(X_valid_poly)\n",
    "    valid_rmse = np.sqrt(mean_squared_error(y_valid_poly, y_pred_valid))\n",
    "    return train_rmse, valid_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "* Read the test.csv file into a dataframe\n",
    "* Feature engineer the dataframe in exactly the same way as above by using function `feature_engineering`.\n",
    "* Scale the quantitative variables the same way as above\n",
    "* Train a model\n",
    "* Predict\n",
    "* Convert the predictions using exponential (since our model is built using log for the target variable)\n",
    "* Create a dataframe for the results with the right format\n",
    "* Save it into csv file and submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Types of Classifiers\n",
    "\n",
    "\n",
    "As we learned in the previous session, the equation for simple linear regression is \n",
    "\n",
    "$$y_{pred} = w*x+b $$\n",
    "\n",
    "Similarly, the linear regression for more than one features, say $x_1, x_2, \\dots, x_n$ is given by\n",
    "\n",
    "$$y_{pred} = w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b$$\n",
    "\n",
    "The learning process for this model involves learning the weights (or parameters) $w_1, w_2, \\dots, w_n$ and $b$ by minimizing the cost function defined by mean-squared error. The weights are iteratively updated using the gradient descent algorithm to minimize the cost function in the fastest way.\n",
    "\n",
    "#### Logistic Regression:\n",
    "Fits a linear decision boundary to separate the classes. \n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/f663cd4f29335972950dded4d422c07aeee8af55/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"250\" />\n",
    "<p style=\"text-align: center;\"> Logistic Regression classifier </p>\n",
    "\n",
    "For binary classification, we assign the two classes the labels 0 and 1. The class labeled 1 is also called the positive class. The classifier predicts the probability ($p$) that an observation belongs to the positive class. The probability for the class labeled $0$ (or the negative class) would be $1-p$.\n",
    "\n",
    "To build a linear classifier is same as finding a function for probability that gives a value close to 1 for points in the upper region (or the points in the positive class) and a value close to 0 for points in the lower region (or the points in the negative class). \n",
    "\n",
    "In the context of neural networks, such a function is called an activation function. They are said to be fired or not depending on whether $f \\to 1$ or $f \\to 0$ given some input for each node or neuron. This activation function is the sigmoid, among the more popular one like RELU.\n",
    "\n",
    "$$sig(t) = \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Sigmoid-function-2.svg\" width=400 />\n",
    "\n",
    "Our main objective for a classification task is to find the optimal decision boundary to separate the classes. For the logistic regression, the boundary is linear. For the case of two features, this linear boundary is simply a line in 2-dimensional plane, whereas for three features, the linear boundary would be a linear plane separating the two classes in 3-dimensional plane and in general, a $n-1$ dimensional linear hyperplane in a $n$-dimensional space.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/f663cd4f29335972950dded4d422c07aeee8af55/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"250\" />\n",
    "<p style=\"text-align: center;\"> Logistic Regression classifier </p>\n",
    "\n",
    "To be able to visualize and understand intuitively, we will first crack the formulation of logistic classifier, also known by its misnomer logistic regression, in the case of two features, say $x_1$ and $x_2$, as seen in the figure above.\n",
    "\n",
    "Math question: We represent the following line using the equation $x_1-x_2-1=0$. How do we mathematically represent the two regions that are separated by this line?\n",
    "\n",
    "![](https://github.com/AashitaK/ML-Workshops/blob/master/Session%204/figures/fig1.png?raw=true)\n",
    "\n",
    "The region containing the origin is given by $x_1-x_2-1<0$ whereas the other one by $x_1-x_2-1>0$.\n",
    "\n",
    "![](https://github.com/AashitaK/ML-Workshops/blob/master/Session%204/figures/fig2.png?raw=true)\n",
    "\n",
    "As seen above, the points in one region is characterized by $w_1*x_1 + w_2*x_2+b<0$ and the other region by $w_1*x_1 + w_2*x_2+b>0$. We combine this with the logistic (sigmoid) function above to get the equation for logistic regression:\n",
    "\n",
    "$$Prob(y=1) = sig(w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b) $$ \n",
    "\n",
    "where $sig$ is the sigmoid logistic function defined above. \n",
    "\n",
    "Observations:\n",
    "* The output of the sigmoid lies between 0 and 1, which corresponds to the probability in our case. \n",
    "* The logistic function (and hence the probability) approximates to 1 for the large positive values, whereas it converges to 0 for large negative values. \n",
    "* The value for $w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b$ is positive for points in the region on one side of the line and negative for the other. The magnitude of the values (positive or negative) is higher for points far away from the line.\n",
    "* In view of the above equation for logistic regression and the properties of sigmoid logistic function, the points farther away from the line will be classified with a high probability to one class or the other, whereas the probability will be closer to 0 for points close to the line.\n",
    " \n",
    "In general, we set the threshold for probability to be 0.5. This means that whenever $w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b \\geq 0$, it is classified to the positive class, whereas whenever $w_1*x_1 + \\cdots + w_n*x_n + b < 0$, it is classified to the negative class. The points for which the value for $w_1*x_1 + \\cdots + w_n*x_n + b$ is not large in magnitude have probabilities that are closer to 0.5. Such points needs to be classified with extra care, as we will see later on in evaluation metrics. \n",
    "\n",
    "The weights $w_1, w_2, \\dots, w_n$ and $b$ are learned by minimizing the cost function, as seen in the previous session. The cost function used is called the cross-entropy log loss and is defined below.\n",
    "\n",
    "For points with label $y=1$, the cost is\n",
    "\n",
    "$$ c(y, p) = - \\log(p) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{ if }\\ \\  y = 1$$\n",
    "\n",
    "whereas for points with label $y=0$, the cost is\n",
    "\n",
    "$$ c(y, p) = - \\log(1-p) \\ \\  \\text{ if }\\ \\  y = 0$$\n",
    "\n",
    "The cost function takes the average over the costs for all points. The costs for the two classes $y=0$ and $y=1$ can be summed up in the following formula.\n",
    "\n",
    "$$ J = \\frac{1}{N} \\sum_{i=1}^N c(y, p) = - \\frac{1}{N} \\sum_{i=1}^N y \\log(p) + (1-y) \\log(1-p)$$\n",
    "\n",
    "where $p=Prob(y=1)$.\n",
    "\n",
    "The updates to the weights are made in a similar fashion as seen earlier for linear regression by minimizing the cost function using gradient descent algorithm.\n",
    "\n",
    "<img src=\"https://uniformlyuninformative.files.wordpress.com/2014/05/lecture6-logistic-regression_slide19.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first generate a dataset suitable for demonstration and applying classification algorithms using built-in function [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=2, \n",
    "                           n_redundant=0, n_informative=2, \n",
    "                           n_clusters_per_class=2, \n",
    "                           random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary classification dataset containing two features (or variables) is plotted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n",
    "            s=25, edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the dataset into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a logistic classifier using [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the classifier using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test the accuracy of the classifier on both training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(LR_clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(LR_clf.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function to plot the decision boundaries of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    x1, x2 = X[:, 0], X[:, 1]\n",
    "    x1_min, x1_max = x1.min() - 1, x1.max() + 1\n",
    "    x2_min, x2_max = x2.min() - 1, x2.max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1),\n",
    "                         np.arange(x2_min, x2_max, 0.1))\n",
    "\n",
    "    Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4)\n",
    "    plt.scatter(x1, x2, c=y, marker='o',\n",
    "                s=25, edgecolor='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(LR_clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above binary classification dataset was clearly separable by linear boundary, but that is not the case often. Next, we generate another dataset that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "X1, y1 = make_gaussian_quantiles(n_features=2, \n",
    "                                 n_classes=2, \n",
    "                                 random_state=12)\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=y1,\n",
    "            s=25, edgecolor='k');\n",
    "\n",
    "X1_train, X1_valid, y1_train, y1_valid = train_test_split(X1, y1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a logistic regression model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_clf2 = LogisticRegression().fit(X1_train, y1_train)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(LR_clf2.score(X1_train, y1_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(LR_clf2.score(X1_valid, y1_valid)))\n",
    "\n",
    "plot_decision_boundary(LR_clf2, X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is perhaps not a good choice for this kind dataset. We first test this dataset with other classification algorithms, namely:\n",
    "- Decision Trees\n",
    "- k-Nearest Neighbors\n",
    "- Support Vector Machines (SVM)\n",
    "- Random Forests\n",
    "\n",
    "These algorithms are briefly introduced here first and we will revisit to learn more about them in the exercise session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Decision Tree classifier](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052):\n",
    "\n",
    "- Tree-like model of decisions\n",
    "- Also known as CART algorithm\n",
    "- Decision making is explicit and can be visualized\n",
    "- Sensitive to the training data used. Slight changes in the training can lead to vastly different trees. This is one of the reasons we prefer Random Forest ensemble given below.\n",
    "- Can create over-complex trees that over-fit to the training data\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\" width=\"300\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Decision Tree classifier for Titanic dataset </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT_clf = DecisionTreeClassifier().fit(X1_train, y1_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(DT_clf.score(X1_train, y1_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(DT_clf.score(X1_valid, y1_valid)))\n",
    "\n",
    "plot_decision_boundary(DT_clf, X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors classifier:\n",
    "- Uses k-nearest neighbors from the training data to predict the label \n",
    "- Unsupervised learning\n",
    "\n",
    "![](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/03faee64-e85e-4ea0-a2b4-e5964949e2d1/d99b9a4d-618c-45f0-86d1-388bdf852c1d/images/screenshot.gif)\n",
    "\n",
    "The default value for k is 5 in scikit-learn implementation.\n",
    "4 Steps:\n",
    "1. Randomly initialize k clusters centroids\n",
    "2. For each training example, assign prediction to its closest cluster centroid.\n",
    "3. For each cluster, set position of cluster centroid to average(mean) of points assigned to that cluster.\n",
    "4. Iterate 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier().fit(X1_train, y1_train)\n",
    "\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn_clf.score(X1_train, y1_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn_clf.score(X1_valid, y1_valid)))\n",
    "\n",
    "plot_decision_boundary(knn_clf, X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM):\n",
    "- An extension of linear classification\n",
    "- Separates the classes by choosing a hyperplane that maximizes the distance to the nearest data point on either side and hence known as maximum margin classifier.\n",
    "- The data points that are closest to the decision boundary on either side are called support vectors.\n",
    "- It minimize the classification error and at the same time maximizes the geometric margin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png\" width=\"350\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Support Vector Machines (SVM) classifier </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the kernel trick to transform the feature space, SVM can learn non-linear decision boundaries. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1b/Kernel_Machine.png\" width=\"350\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Using Kernel trick for Support Vector Machines (SVM) classifier </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_clf = SVC().fit(X1_train, y1_train)\n",
    "\n",
    "print('Accuracy of Support Vector classifier on training set: {:.2f}'\n",
    "     .format(svc_clf.score(X1_train, y1_train)))\n",
    "print('Accuracy of Support Vector classifier on test set: {:.2f}'\n",
    "     .format(svc_clf.score(X1_valid, y1_valid)))\n",
    "\n",
    "plot_decision_boundary(svc_clf, X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Random Forest ensemble](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/): \n",
    "- Ensemble of decision tree classifiers which means it combines predictions from multiple decision tree classifiers to give a more accurate prediction than any individual tree.\n",
    "- As noted earlier, decision trees are unstable, that is they are sensitive to the changes in training data. For the ensemble, we take bootstrap samples from the training data and built trees on it.\n",
    "- Prone to over-fitting, though less prone than individual decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_clf = RandomForestClassifier(random_state=0).fit(X1_train, y1_train)\n",
    "\n",
    "print('Accuracy of Random Forest classifier on training set: {:.2f}'\n",
    "     .format(RF_clf.score(X1_train, y1_train)))\n",
    "print('Accuracy of Random Forest classifier on test set: {:.2f}'\n",
    "     .format(RF_clf.score(X1_valid, y1_valid)))\n",
    "\n",
    "plot_decision_boundary(RF_clf, X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoother boundary usually suggests that it is less likely to be subjected to the noise and fluctations in training set and hence, less likely to over-fit. \n",
    "\n",
    "For this dataset:\n",
    "* The support vector classifier with minimal difference between training and testing set accuracy and smoother decision boundary seems to have performed the best\n",
    "* The decision tree algorithm seems to have suffered the most from over-fitting. \n",
    "* The logistic regression classifier seems to be under-fitting on account of being a poor fit for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The promise of neural networks is that they can learn almost any kind of decision boundary if we build our network deep enough with suitable architecture and train it long enough with sufficiently big dataset. All other methods have limitations in how complex of a boundary they can learn. However, unless we have an abundance of training examples and the computational power, simpler methods are a good answer to many problems in industry even today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Classifiers\n",
    "\n",
    "This is the exercise notebook for the fourth session of the [Machine Learning workshop series at Harvey Mudd College](http://www.aashitak.com/ML-Workshops/). Please feel free to ask for help from the instructor and/or TAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's exercise, we will work with the [Titanic dataset from Kaggle](https://www.kaggle.com/c/titanic). The objective of this Kaggle competition is to predict whether a passenger survives or not given a number of features related to passengers' information such as gender, age, ticket class, etc. We are going to build a few classification models to predict whether a passenger survives. The `train.csv` file contains features along with the information about the survival of the passenger, so we will use it to train and validate our models. The `test.csv` file contains only features and we will use one of our trained models to predict the survival for these passengers and [submit our predictions to the competitions leaderboard](https://www.kaggle.com/c/titanic/submit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, the data preprocessing and feature engineering that we did in the previous sessions is summarized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'titanic/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-ba182378bdf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'titanic/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSurvived\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'titanic/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "path = 'titanic/'\n",
    "df = pd.read_csv(path + 'train.csv')\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "target = train.Survived.astype('category', ordered=False)\n",
    "train.drop('Survived', axis=1)\n",
    "\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "PassengerId = test.PassengerId\n",
    "\n",
    "def get_Titles(df):\n",
    "    df.Name = df.Name.apply(lambda name: re.findall(\"\\s\\S+[.]\\s\", name)[0].strip())\n",
    "    df = df.rename(columns = {'Name': 'Title'})\n",
    "    df.Title.replace({'Ms.': 'Miss.', 'Mlle.': 'Miss.', 'Dr.': 'Rare', 'Mme.': 'Mr.', 'Major.': 'Rare', 'Lady.': 'Rare', 'Sir.': 'Rare', 'Col.': 'Rare', 'Capt.': 'Rare', 'Countess.': 'Rare', 'Jonkheer.': 'Rare', 'Dona.': 'Rare', 'Don.': 'Rare', 'Rev.': 'Rare'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def fill_Age(df):\n",
    "    df.Age = df.Age.fillna(df.groupby(\"Title\").Age.transform(\"median\"))\n",
    "    return df\n",
    "\n",
    "def get_Group_size(df):\n",
    "    Ticket_counts = df.Ticket.value_counts()\n",
    "    df['Ticket_counts'] = df.Ticket.apply(lambda x: Ticket_counts[x])\n",
    "    df['Family_size'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['Group_size'] = df[['Family_size', 'Ticket_counts']].max(axis=1)\n",
    "    return df\n",
    "\n",
    "def process_features(df):\n",
    "    df.Sex = df.Sex.astype('category', ordered=False).cat.codes\n",
    "    features_to_keep = ['Age', 'Fare', 'Group_size', 'Pclass', 'Sex']\n",
    "    df = df[features_to_keep]\n",
    "    return df\n",
    "\n",
    "def process_data(df):\n",
    "    df = df.copy()\n",
    "    df = get_Titles(df)\n",
    "    df = fill_Age(df)\n",
    "    df = get_Group_size(df)\n",
    "    df = process_features(df)\n",
    "    medianFare = df['Fare'].median()\n",
    "    df['Fare'] = df['Fare'].fillna(medianFare)\n",
    "    return df\n",
    "\n",
    "X_train, X_test = process_data(train), process_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please feel free to refer to the classification algorithms notebook for the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, split the data into training and validation set using `train_test_split` and name the variables as `X_train, X_valid, y_train, y_valid `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression classifier on `X_train, y_train` and test its accuracy on both `X_train, y_train` and `X_valid, y_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_clf = LogisticRegression()\n",
    "LR_clf.fit(X_train, y_train)\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(LR_clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on validation set: {:.2f}'\n",
    "     .format(LR_clf.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The evaluation metric for this competition is accuracy](https://www.kaggle.com/c/titanic/overview/evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training  a few more classifiers and compare the accuracy. Try tuning the hyperparameters too. You can also try more feature engineering by editing the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = SVC().fit(X_train, y_train)\n",
    "print('Accuracy of Support Vector classifier on training set: {:.2f}'\n",
    "     .format(svc_clf.score(X_train, y_train)))\n",
    "print('Accuracy of Support Vector classifier on validation set: {:.2f}'\n",
    "     .format(svc_clf.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_MLP(X_train, y_train, input_dim, dropout, epochs, batch_size):\n",
    "    print('-----------Running Multilayer Perceptron-----------')\n",
    "    # Build model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Choose optimizer and loss function\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "    loss = 'binary_crossentropy'\n",
    "    # Compile \n",
    "    model.compile(optimizer=opt, \n",
    "        loss=loss,\n",
    "        metrics=['accuracy'])\n",
    "    # Fit on training data and cross-validate\n",
    "    model.fit(X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Running Multilayer Perceptron-----------\n",
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (5,) but got array with shape (57,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-339c452e8181>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_MLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-373d2a56ecbb>\u001b[0m in \u001b[0;36mrun_MLP\u001b[1;34m(X_train, y_train, input_dim, dropout, epochs, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m     model.fit(X_train, y_train,\n\u001b[0;32m     21\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2382\u001b[1;33m         exception_prefix='input')\n\u001b[0m\u001b[0;32m   2383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[1;34m'Error when checking '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    363\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (5,) but got array with shape (57,)"
     ]
    }
   ],
   "source": [
    "model = run_MLP(X_train, y_train, 5, 0.1, 70, 128)\n",
    "score = model.evaluate(X_valid, y_valid, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters using gridsearch (this is why we have the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have explored a different classifiers and decided on one trained model (or a voting classifer ensemble as seen before), let us use it to make predictions using the features from `X_test` and save the results into `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "y_test = [1 if i >= 0.5 else 0 for i in y_test]\n",
    "y_test = pd.Series((i for i in y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe for submission using the predictions from `y_test` and save it to a csv file. It is important that our submission file is in correct format to be graded without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PassengerId': PassengerId, 'Survived': y_test})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
